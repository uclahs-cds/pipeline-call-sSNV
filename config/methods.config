import nextflow.util.SysHelper

includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/schema/schema.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/retry/retry.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/methods/common_methods.config"
includeConfig "${projectDir}/external/pipeline-Nextflow-config/config/bam/bam_parser.config"

methods {
    set_process = {
        process.cache = params.cache_intermediate_pipeline_steps
    }

    modify_base_allocations = {
        if (!(params.containsKey('base_resource_update') && params.base_resource_update)) {
            return
        }

        params.base_resource_update.each { resource, updates ->
            updates.each { processes, multiplier ->
                def processes_to_update = (custom_schema_types.is_string(processes)) ? [processes] : processes
                methods.update_base_resource_allocation(resource, multiplier, processes_to_update)
            }
        }
    }

    // Function to ensure that resource requirements don't go beyond
    // a maximum limit
    check_max = { obj, type ->
        if (type == 'memory') {
            try {
                if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                    return params.max_memory as nextflow.util.MemoryUnit
                else
                    return obj
            } catch (all) {
                println "   ### ERROR ###   Max memory '${params.max_memory}' is not valid! Using default value: $obj"
                return obj
            }
        } else if (type == 'time') {
            try {
                if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                    return params.max_time as nextflow.util.Duration
                else
                    return obj
            } catch (all) {
                println "   ### ERROR ###   Max time '${params.max_time}' is not valid! Using default value: $obj"
                return obj
            }
        } else if (type == 'cpus') {
            try {
                return Math.min(obj, params.max_cpus as int)
            } catch (all) {
                println "   ### ERROR ###   Max cpus '${params.max_cpus}' is not valid! Using default value: $obj"
                return obj
            }
        }
    }

    // Function to ensure that resource requirements don't go beyond
    // a maximum limit or below a minimum limit
    check_limits = { obj, type ->
        if (type == 'memory') {
            try {
                if (obj.compareTo(params.max_memory as nextflow.util.MemoryUnit) == 1)
                    return params.max_memory as nextflow.util.MemoryUnit
                else if (obj.compareTo(params.min_memory as nextflow.util.MemoryUnit) == -1)
                    return params.min_memory as nextflow.util.MemoryUnit
                else
                    return obj
            } catch (all) {
                println "   ### WARNING ###   Max memory '${params.max_memory}' or min memory '${params.min_memory}' is not valid! Using default value: $obj"
                return obj
            }
        } else if (type == 'time') {
            try {
                if (obj.compareTo(params.max_time as nextflow.util.Duration) == 1)
                    return params.max_time as nextflow.util.Duration
                else if (obj.compareTo(params.min_time as nextflow.util.Duration) == -1)
                    return params.min_time as nextflow.util.Duration
                else
                    return obj
            } catch (all) {
                println "   ### WARNING ###   Max time '${params.max_time}' or min time '${params.min_time}' is not valid! Using default value: $obj"
                return obj
            }
        } else if (type == 'cpus') {
            try {
                return Math.max( Math.min( obj, params.max_cpus as int ), params.min_cpus as int )
            } catch (all) {
                println "   ### WARNING ###   Max cpus '${params.max_cpus}' or min cpus '${params.min_cpus}' is not valid! Using default value: $obj"
                return obj
            }
        }
    }

    sanitize_string = { raw ->
        if (![String, GString].any{ raw in it }) {
            throw new Exception("Input to sanitize is either empty or not a string! Provide a non-empty string.")
            }
        def disallowed_characters = /[^a-zA-Z\d\/_.-]/
        return raw.replaceAll(disallowed_characters, '').replace('_', '-')
    }

    get_ids_from_bams = {
        params.samples_to_process = [] as Set
        params.input.each { k, v ->
            v.each { sampleMap ->
                def bam_path = sampleMap['BAM']
                def bam_header = bam_parser.parse_bam_header(bam_path)
                def sm_tags = bam_header['read_group'].collect{ it['SM'] }.unique()
                if (sm_tags.size() > 1) {
                    throw new Exception("${bam_path} contains multiple samples! Please run pipeline with single sample BAMs.")
                }
                sm_tag = methods.sanitize_string(sm_tags[0])
                params.samples_to_process.add(['orig-id': sm_tags[0], 'id': sm_tag, 'path': bam_path, 'sample_type': k])
            }
        }
    }

    set_sample_params = {
        params.multi_tumor_sample = false
        params.multi_normal_sample = false
        params.tumor_only_mode = false
        params.single_NT_paired = false

        if (!params?['input']?['normal']?['BAM'] ) {
            params.tumor_only_mode = true
            params.input['normal'] = [BAM: "${params.work_dir}/NO_FILE.bam"]
        } else {
            if ( params.input['tumor'].size()  > 1 ) {
                params.multi_tumor_sample = true
            }
            if ( params.input['normal'].size() > 1 ) {
                params.multi_normal_sample = true
            }
        }

        if (params.multi_tumor_sample || params.multi_normal_sample) {
            params.sample_id = params.patient_id
        } else {
            def tumorSample = params.samples_to_process.find { it['sample_type'] == 'tumor' }
            if (tumorSample) {
                params.sample_id = tumorSample['id']
                params.tumor_id = params.sample_id
            } else {
                throw new Exception("Warning: No sample with sample_type 'tumor' found in ${params.samples_to_process}")
            }
            if (params.tumor_only_mode) {
                params.normal_id = 'Empty_id'
            } else {
                params.normal_id = params.samples_to_process.find { it['sample_type'] == 'normal' }['id']
                params.single_NT_paired = true
            }
        }
    }

    set_intersect_regions_params = {
        if (params.containsKey("intersect_regions") && params.intersect_regions) {
            params.intersect_regions_index = "${params.intersect_regions}.tbi"
            params.use_intersect_regions = true
        } else {
            params.intersect_regions = "${params.work_dir}/NO_FILE.bed"
            params.intersect_regions_index = "${params.work_dir}/NO_FILE.bed.tbi"
            params.use_intersect_regions = false
        }
    }

    set_mutect2_params = {
        if (params.containsKey("germline_resource_gnomad_vcf") && params.germline_resource_gnomad_vcf) {
            params.germline = true
        } else {
            params.germline_resource_gnomad_vcf = "${params.work_dir}/NO_FILE.vcf.gz"
            params.germline = false
        }
        params.germline_resource_gnomad_vcf_index = "${params.germline_resource_gnomad_vcf}.tbi"
        // check if contamination estimation table exist and set params.use_contamination_estimation
        params.use_contamination_estimation = false
        if (params?['input']?['tumor']) {
            params.use_contamination_estimation = params['input']['tumor'].any{
                t -> t.containsKey('contamination_table') && t['contamination_table']
            }
        }

        if (!params.use_contamination_estimation) {
            for (tumor in params.input['tumor']){
                tumor.contamination_table = "${params.work_dir}/NO_FILE.table"
            }
        }
    }

    set_output_directory = {
        def tz = TimeZone.getTimeZone("UTC")
        def date = new Date().format("yyyyMMdd'T'HHmmss'Z'", tz)
        params.output_dir_base = "${params.output_dir}/${manifest.name}-${manifest.version}/${params.sample_id}"
        params.log_output_dir = "${params.output_dir_base}/log-${manifest.name}-${manifest.version}-${date}"
    }

    set_pipeline_log = {
        trace.enabled = true
        trace.file = "${params.log_output_dir}/nextflow-log/trace.txt"

        timeline.enabled = true
        timeline.file = "${params.log_output_dir}/nextflow-log/timeline.html"

        report.enabled = true
        report.file = "${params.log_output_dir}/nextflow-log/report.html"
    }

    check_valid_algorithms = {
        Set valid_algorithms = ['somaticsniper', 'strelka2', 'mutect2', 'muse']
        if (params.tumor_only_mode || params.multi_tumor_sample || params.multi_normal_sample ) {
            valid_algorithms = ['mutect2']
        }
        for (algo in params.algorithm) {
            if (!(algo in valid_algorithms)) {
                if (params.tumor_only_mode || params.multi_tumor_sample || params.multi_normal_sample ) {
                    throw new Exception("ERROR: params.algorithm ${params.algorithm} contains an invalid value. Tumor-only mode or multi-sample mode is only applied to Mutect2 algorithm.")
                } else {
                    throw new Exception("ERROR: params.algorithm ${params.algorithm} contains an invalid value.")
                }
            }
        }
    }
    setup = {
        schema.load_custom_types("${projectDir}/config/custom_schema_types.config")
        schema.validate()
        methods.set_process()
        methods.set_resources_allocation()
        methods.modify_base_allocations()
        retry.setup_retry()
        methods.set_env()
        methods.get_ids_from_bams()
        methods.set_sample_params()
        methods.set_intersect_regions_params()
        methods.set_mutect2_params()
        methods.set_output_directory()
        methods.set_pipeline_log()
        methods.check_valid_algorithms()
        methods.setup_docker_cpus()
    }
}
